# ðŸ¤– Offline AI Code Generator

A fully offline AI code generation tool powered by **CodeLlama-7B** with **RAG** (Retrieval-Augmented Generation) for context-aware code generation.

## Features

- **Fully Offline** â€” After initial model download, works without internet
- **CodeLlama-7B** â€” Meta's code-specialized LLM with 4-bit quantization support
- **RAG-Enhanced** â€” ChromaDB + local embeddings for context-aware generation
- **Chat Mode** â€” Conversational interface like ChatGPT (remembers context)
- **Project Generation** â€” Generate complete project structures with all files
- **Token Management** â€” Automatic chunked generation for large files
- **Knowledge Base** â€” Add your own code/docs for better context

## Requirements

- Python 3.9+
- 8GB+ RAM (16GB recommended)
- NVIDIA GPU with 6GB+ VRAM (optional, CPU works but slower)
- ~15GB disk space for model

## Quick Start

```bash
# 1. Run setup (requires internet ONCE)
chmod +x setup.sh
./setup.sh

# 2. Start the generator (fully offline)
source venv/bin/activate
python main.py
```

## Usage

### Interactive Chat
```
ðŸ’¬ You: Write a Python function to merge two sorted lists
ðŸ¤– Assistant: [generates code]

ðŸ’¬ You: Now add type hints and docstring
ðŸ¤– Assistant: [generates improved code with context]
```

### Generate Full Projects
```
ðŸ’¬ You: /project A Flask REST API for a todo app with SQLite database
```

### Add Knowledge
```
ðŸ’¬ You: /add /path/to/your/codebase/utils.py
```

## Project Structure

```
offline-code-generator/
â”œâ”€â”€ main.py              # Entry point & CLI interface
â”œâ”€â”€ config.py            # All configuration settings
â”œâ”€â”€ model_loader.py      # CodeLlama model loading & quantization
â”œâ”€â”€ rag_engine.py        # RAG with ChromaDB & local embeddings
â”œâ”€â”€ code_generator.py    # Code generation & token management
â”œâ”€â”€ project_generator.py # Project file writer
â”œâ”€â”€ setup.sh             # One-time setup script
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ models/              # Downloaded models (created by setup)
â”œâ”€â”€ knowledge_base/      # Your reference code/docs
â”œâ”€â”€ data/                # ChromaDB vector store
â””â”€â”€ generated_projects/  # Output directory
```

## Configuration

Edit `config.py` to customize:

| Setting | Default | Description |
|---------|---------|-------------|
| `USE_4BIT_QUANTIZATION` | `True` | Use 4-bit quantization (saves VRAM) |
| `DEVICE` | `cuda` | `cuda` for GPU, `cpu` for CPU-only |
| `MAX_NEW_TOKENS` | `2048` | Max tokens per generation |
| `TEMPERATURE` | `0.2` | Lower = more deterministic |
| `RAG_TOP_K` | `5` | Number of context chunks to retrieve |

## How It Works

1. **Model**: CodeLlama-7B-Instruct runs locally with optional 4-bit quantization
2. **RAG**: Your prompts are matched against a local ChromaDB vector store containing code patterns, project structures, and conversation history
3. **Token Management**: Long files are generated in chunks; the system detects incomplete code and continues automatically
4. **Memory**: Conversation history is stored both in-memory and in the vector store for long-term recall


# 